{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg5GvKa0qXkT"
   },
   "source": [
    "# Установка нужных библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7P4TWOOmqXkY"
   },
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T12:20:27.982302Z",
     "start_time": "2024-04-03T12:20:23.962189Z"
    },
    "id": "4EVJmkwOqXkY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 02:19:33.672551: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-11 02:19:34.618404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from src.mylib.models.models import Model\n",
    "from src.mylib.train import Trainer\n",
    "\n",
    "file = os.path.abspath('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2HeCQ89qXkZ"
   },
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Начальные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T12:20:27.986925Z",
     "start_time": "2024-04-03T12:20:27.983711Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Длина окна\n",
    "window_length_seconds = 5 \n",
    "sample_rate = 64\n",
    "window_length = window_length_seconds * sample_rate\n",
    "\n",
    "# Расстояние между двумя окнами\n",
    "hop_length_seconds = 1\n",
    "hop_length = sample_rate * hop_length_seconds\n",
    "\n",
    "# Количество ложные стимулов\n",
    "number_of_mismatch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T12:20:27.999209Z",
     "start_time": "2024-04-03T12:20:27.988090Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_folder = os.path.dirname(file)\n",
    "\n",
    "# Load the config file\n",
    "with open(os.path.join(experiment_folder, \"src/mylib/utils/config.json\")) as file_path:\n",
    "    config = json.load(file_path)\n",
    "\n",
    "# Path to the dataset, which is already split to train, val, test\n",
    "data_folder = os.path.join(config[\"dataset_folder\"], config['derivatives_folder'], config[\"split_folder\"])\n",
    "\n",
    "# Пути к тренировочным, валидационным и тестовым данным\n",
    "train_files = [x for x in glob.glob(os.path.join(data_folder, \"train_-_*\")) if\n",
    "                       os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in [\"eeg\", \"envelope\"]]\n",
    "val_files = [x for x in glob.glob(os.path.join(data_folder, \"val_-_*\")) if\n",
    "                       os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in [\"eeg\", \"envelope\"]]\n",
    "test_files = [x for x in glob.glob(os.path.join(data_folder, \"test_-_*\")) if\n",
    "                       os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in [\"eeg\", \"envelope\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19nb_usNqXkc"
   },
   "source": [
    "## Обучение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T12:30:52.236219Z",
     "start_time": "2024-04-03T12:20:28.000137Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMK7mqNQZPXJ",
    "outputId": "a95524d6-db85-4a34-9c36-fa2befec2f34"
   },
   "outputs": [],
   "source": [
    "args = {\"window_length\" : window_length, \"hop_length\" : hop_length, \"number_of_mismatch\" : number_of_mismatch, \"batch_size\" : batch_size, \n",
    "        \"max_files\" : None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовый энкодер ЭЭГ + базовый энкодер стимула"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "EPOCH 1:\n",
      "  batch 100 loss: 0.14436513595159414\n",
      "  batch 200 loss: 0.0009197634062729498\n",
      "  batch 300 loss: 0.00022809133013353743\n",
      "  batch 400 loss: 9.146743850525495e-06\n",
      "LOSS train 9.146743850525495e-06 valid 0.00019639057730306194\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.00017933270480170683\n",
      "  batch 200 loss: 1.6600384697085246e-05\n",
      "  batch 300 loss: 3.069836762728317e-05\n",
      "  batch 400 loss: 4.906420497619024e-07\n",
      "LOSS train 4.906420497619024e-07 valid 0.0002756381290764012\n",
      "Testing\n",
      "sub-033\n",
      "    Mean accuracy per subject: 99.11373707533235\n",
      "sub-011\n",
      "    Mean accuracy per subject: 99.27360774818402\n",
      "sub-014\n",
      "    Mean accuracy per subject: 99.6319018404908\n",
      "sub-024\n",
      "    Mean accuracy per subject: 99.27360774818402\n",
      "sub-078\n",
      "    Mean accuracy per subject: 99.08925318761385\n",
      "sub-064\n",
      "    Mean accuracy per subject: 99.21752738654148\n",
      "sub-073\n",
      "    Mean accuracy per subject: 98.72495446265938\n",
      "sub-042\n",
      "    Mean accuracy per subject: 98.53420195439739\n",
      "sub-021\n",
      "    Mean accuracy per subject: 99.38650306748467\n",
      "sub-076\n",
      "    Mean accuracy per subject: 98.90710382513662\n",
      "sub-013\n",
      "    Mean accuracy per subject: 99.15254237288136\n",
      "sub-075\n",
      "    Mean accuracy per subject: 98.72495446265938\n",
      "sub-005\n",
      "    Mean accuracy per subject: 99.46595460614152\n",
      "sub-001\n",
      "    Mean accuracy per subject: 99.15254237288136\n",
      "sub-036\n",
      "    Mean accuracy per subject: 99.11373707533235\n",
      "sub-022\n",
      "    Mean accuracy per subject: 99.38650306748467\n",
      "sub-017\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-062\n",
      "    Mean accuracy per subject: 98.19967266775778\n",
      "sub-072\n",
      "    Mean accuracy per subject: 99.08925318761385\n",
      "sub-029\n",
      "    Mean accuracy per subject: 98.25949367088607\n",
      "sub-085\n",
      "    Mean accuracy per subject: 99.10394265232975\n",
      "sub-030\n",
      "    Mean accuracy per subject: 99.0506329113924\n",
      "Score:  99.08416487924477\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "trainer = Trainer(\n",
    "    model, train_files, val_files, test_files, args, torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    ")\n",
    "print(\"Training\")\n",
    "trainer.train_model(epochs=2, run_name=\"Baseline\", eps=1e-5)\n",
    "print(\"Testing\")\n",
    "trainer.test(window_length, hop_length, number_of_mismatch, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформер для ЭЭГ + базовый энкодер стимула"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "EPOCH 1:\n",
      "  batch 100 loss: 0.14217073852825343\n",
      "  batch 200 loss: 0.0021196607220531918\n",
      "  batch 300 loss: 0.0012117775476999603\n",
      "  batch 400 loss: 0.00012176215706791283\n",
      "  batch 500 loss: 0.0006003972515463829\n",
      "  batch 600 loss: 0.0008171536028385162\n",
      "  batch 700 loss: 0.0005988744071510155\n",
      "  batch 800 loss: 1.7175454407531985e-05\n",
      "  batch 900 loss: 2.066788147097043e-06\n",
      "LOSS train 2.066788147097043e-06 valid 1.7467377588164068e-10\n",
      "Testing\n",
      "sub-013\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-064\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-005\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-021\n",
      "    Mean accuracy per subject: 99.87730061349693\n",
      "sub-078\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-024\n",
      "    Mean accuracy per subject: 99.87893462469734\n",
      "sub-042\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-085\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-022\n",
      "    Mean accuracy per subject: 99.87730061349693\n",
      "sub-036\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-001\n",
      "    Mean accuracy per subject: 99.87893462469734\n",
      "sub-062\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-073\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-017\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-014\n",
      "    Mean accuracy per subject: 99.87730061349693\n",
      "sub-033\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-076\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-011\n",
      "    Mean accuracy per subject: 99.87893462469734\n",
      "sub-072\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-075\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-029\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-030\n",
      "    Mean accuracy per subject: 100.0\n",
      "Score:  99.96675935066287\n"
     ]
    }
   ],
   "source": [
    "model = Model(use_transformer=True)\n",
    "trainer = Trainer(\n",
    "    model, train_files, val_files, test_files, args, torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    ")\n",
    "print(\"Training\")\n",
    "trainer.train_model(epochs=2, run_name=\"Transformer\", eps=1e-5)\n",
    "print(\"Testing\")\n",
    "trainer.test(window_length, hop_length, number_of_mismatch, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовый энкодер ЭЭГ + Эмбеддинги `Wav2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "EPOCH 1:\n",
      "  batch 100 loss: 0.14670967061872106\n",
      "  batch 200 loss: 0.0002088138647377491\n",
      "  batch 300 loss: 0.0004688701720472466\n",
      "  batch 400 loss: 2.47868995427325e-05\n",
      "  batch 500 loss: 5.7140075941566334e-05\n",
      "  batch 600 loss: 2.771553678115879e-08\n",
      "  batch 700 loss: 5.164515732758446e-07\n",
      "  batch 800 loss: 0.00010941589002527507\n",
      "  batch 900 loss: 1.2982928008664629e-06\n",
      "  batch 1000 loss: 5.7820174260996284e-05\n",
      "  batch 1100 loss: 0.0002700865108567985\n",
      "  batch 1200 loss: 0.001713825878876527\n",
      "  batch 1300 loss: 9.78196553211319e-05\n",
      "  batch 1400 loss: 6.904328486312395e-07\n",
      "  batch 1500 loss: 0.0001654549483994916\n",
      "  batch 1600 loss: 2.2600606383402777e-05\n",
      "  batch 1700 loss: 2.232706955922481e-05\n",
      "  batch 1800 loss: 5.270355261099979e-07\n",
      "  batch 1900 loss: 5.773793950439199e-05\n",
      "  batch 2000 loss: 1.918154739541933e-06\n",
      "  batch 2100 loss: 1.1456513675511814e-07\n",
      "  batch 2200 loss: 0.0\n",
      "  batch 2300 loss: 9.685748336707433e-10\n",
      "  batch 2400 loss: 3.917019967047963e-08\n",
      "  batch 2500 loss: 0.0\n",
      "  batch 2600 loss: 0.0\n",
      "  batch 2700 loss: 0.0\n",
      "  batch 2800 loss: 0.00015420199371874333\n",
      "  batch 2900 loss: 0.0\n",
      "  batch 3000 loss: 5.774197120445024e-10\n",
      "  batch 3100 loss: 1.1895582247234415e-07\n",
      "  batch 3200 loss: 6.645575922448188e-07\n",
      "  batch 3300 loss: 0.0\n",
      "  batch 3400 loss: 3.212476724456792e-07\n",
      "  batch 3500 loss: 0.0\n",
      "  batch 3600 loss: 5.923183719058755e-09\n",
      "  batch 3700 loss: 0.0\n",
      "  batch 3800 loss: 7.04773876350373e-06\n",
      "  batch 3900 loss: 0.0\n",
      "  batch 4000 loss: 3.911554236069037e-10\n",
      "  batch 4100 loss: 0.0\n",
      "  batch 4200 loss: 0.0\n",
      "  batch 4300 loss: 0.0\n",
      "  batch 4400 loss: 1.2256105037522502e-08\n",
      "  batch 4500 loss: 0.0\n",
      "  batch 4600 loss: 8.940690321423972e-10\n",
      "  batch 4700 loss: 0.0\n",
      "  batch 4800 loss: 9.760210559761617e-09\n",
      "  batch 4900 loss: 0.0\n",
      "  batch 5000 loss: 2.2351738238057807e-10\n",
      "  batch 5100 loss: 7.152528382903256e-09\n",
      "  batch 5200 loss: 0.0\n",
      "  batch 5300 loss: 1.5806283045094461e-07\n",
      "  batch 5400 loss: 0.0\n",
      "  batch 5500 loss: 1.2665974225001263e-09\n",
      "  batch 5600 loss: 1.3977693015476687e-07\n",
      "  batch 5700 loss: 0.0\n",
      "  batch 5800 loss: 1.9607731883297674e-06\n",
      "  batch 5900 loss: 1.4206922787707298e-06\n",
      "  batch 6000 loss: 7.779289036989212e-05\n",
      "  batch 6100 loss: 1.6201816379179946e-07\n",
      "  batch 6200 loss: 1.8385057319392217e-06\n",
      "  batch 6300 loss: 7.078050767717059e-10\n",
      "  batch 6400 loss: 8.0814678221941e-06\n",
      "  batch 6500 loss: 1.4081518884268008e-08\n",
      "  batch 6600 loss: 4.7869804120637126e-09\n",
      "  batch 6700 loss: 2.9802315282267954e-10\n",
      "  batch 6800 loss: 2.309677427092538e-09\n",
      "  batch 6900 loss: 0.0\n",
      "  batch 7000 loss: 6.636252237512963e-08\n",
      "  batch 7100 loss: 4.097817907222634e-10\n",
      "  batch 7200 loss: 3.352760558072987e-10\n",
      "  batch 7300 loss: 7.450576333667413e-10\n",
      "  batch 7400 loss: 4.2840831326884655e-10\n",
      "  batch 7500 loss: 9.499482445107787e-10\n",
      "  batch 7600 loss: 0.0\n",
      "  batch 7700 loss: 2.9354821434735e-08\n",
      "  batch 7800 loss: 7.450580152834618e-11\n",
      "  batch 7900 loss: 0.0\n",
      "  batch 8000 loss: 2.2418513253796846e-07\n",
      "  batch 8100 loss: 4.9284598160337366e-08\n",
      "  batch 8200 loss: 2.421438338728876e-10\n",
      "  batch 8300 loss: 4.334225195634644e-08\n",
      "  batch 8400 loss: 5.4389101755702995e-09\n",
      "  batch 8500 loss: 3.911554102842274e-10\n",
      "  batch 8600 loss: 2.0258639779058286e-07\n",
      "  batch 8700 loss: 2.9057228800866143e-09\n",
      "  batch 8800 loss: 0.0\n",
      "  batch 8900 loss: 0.0\n",
      "  batch 9000 loss: 4.65661229576142e-10\n",
      "  batch 9100 loss: 0.0\n",
      "  batch 9200 loss: 1.7285261435517894e-08\n",
      "  batch 9300 loss: 2.2241782062337735e-07\n",
      "  batch 9400 loss: 8.195632972274325e-10\n",
      "  batch 9500 loss: 0.0\n",
      "LOSS train 0.0 valid 2.0441521939703525e-07\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 2.6076813242070786e-09\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 1.1138221225337475e-08\n",
      "  batch 400 loss: 4.023262643038095e-09\n",
      "  batch 500 loss: 7.450579708745409e-11\n",
      "  batch 600 loss: 0.0\n",
      "  batch 700 loss: 0.0\n",
      "  batch 800 loss: 0.0\n",
      "  batch 900 loss: 0.0\n",
      "  batch 1000 loss: 1.763879513561051e-08\n",
      "  batch 1100 loss: 2.396906415891298e-07\n",
      "  batch 1200 loss: 1.2039947449249412e-07\n",
      "  batch 1300 loss: 9.648219836400073e-09\n",
      "  batch 1400 loss: 2.607702609402907e-10\n",
      "  batch 1500 loss: 3.501746448364429e-09\n",
      "  batch 1600 loss: 3.777221898704219e-08\n",
      "  batch 1700 loss: 2.555362561995622e-08\n",
      "  batch 1800 loss: 1.1175868896984298e-10\n",
      "  batch 1900 loss: 3.170095082438706e-08\n",
      "  batch 2000 loss: 5.979062507321942e-09\n",
      "  batch 2100 loss: 0.0\n",
      "  batch 2200 loss: 0.0\n",
      "  batch 2300 loss: 0.0\n",
      "  batch 2400 loss: 0.0\n",
      "  batch 2500 loss: 0.0\n",
      "  batch 2600 loss: 0.0\n",
      "  batch 2700 loss: 0.0\n",
      "  batch 2800 loss: 2.421391855023103e-08\n",
      "  batch 2900 loss: 0.0\n",
      "  batch 3000 loss: 0.0\n",
      "  batch 3100 loss: 0.0\n",
      "  batch 3200 loss: 7.636798500243459e-09\n",
      "  batch 3300 loss: 0.0\n",
      "  batch 3400 loss: 9.313224857976365e-11\n",
      "  batch 3500 loss: 0.0\n",
      "  batch 3600 loss: 0.0\n",
      "  batch 3700 loss: 0.0\n",
      "  batch 3800 loss: 1.862644971595273e-10\n",
      "  batch 3900 loss: 0.0\n",
      "  batch 4000 loss: 0.0\n",
      "  batch 4100 loss: 0.0\n",
      "  batch 4200 loss: 0.0\n",
      "  batch 4300 loss: 0.0\n",
      "  batch 4400 loss: 0.0\n",
      "  batch 4500 loss: 0.0\n",
      "  batch 4600 loss: 0.0\n",
      "  batch 4700 loss: 0.0\n",
      "  batch 4800 loss: 0.0\n",
      "  batch 4900 loss: 0.0\n",
      "  batch 5000 loss: 0.0\n",
      "  batch 5100 loss: 0.0\n",
      "  batch 5200 loss: 0.0\n",
      "  batch 5300 loss: 0.0\n",
      "  batch 5400 loss: 0.0\n",
      "  batch 5500 loss: 0.0\n",
      "  batch 5600 loss: 0.0\n",
      "  batch 5700 loss: 0.0\n",
      "  batch 5800 loss: 2.8870979207340495e-09\n",
      "  batch 5900 loss: 3.257721346017206e-08\n",
      "  batch 6000 loss: 7.450580152834618e-11\n",
      "  batch 6100 loss: 1.7192074324157146e-08\n",
      "  batch 6200 loss: 3.874491812361214e-07\n",
      "  batch 6300 loss: 0.0\n",
      "  batch 6400 loss: 1.0114122233062516e-08\n",
      "  batch 6500 loss: 5.587935003603661e-11\n",
      "  batch 6600 loss: 0.0\n",
      "  batch 6700 loss: 0.0\n",
      "  batch 6800 loss: 0.0\n",
      "  batch 6900 loss: 0.0\n",
      "  batch 7000 loss: 1.6577520511873444e-09\n",
      "  batch 7100 loss: 0.0\n",
      "  batch 7200 loss: 0.0\n",
      "  batch 7300 loss: 0.0\n",
      "  batch 7400 loss: 0.0\n",
      "  batch 7500 loss: 0.0\n",
      "  batch 7600 loss: 0.0\n",
      "  batch 7700 loss: 5.587935003603661e-11\n",
      "  batch 7800 loss: 0.0\n",
      "  batch 7900 loss: 0.0\n",
      "  batch 8000 loss: 1.4901159417490818e-10\n",
      "  batch 8100 loss: 2.924347768384905e-09\n",
      "  batch 8200 loss: 0.0\n",
      "  batch 8300 loss: 7.450580152834618e-11\n",
      "  batch 8400 loss: 0.0\n",
      "  batch 8500 loss: 0.0\n",
      "  batch 8600 loss: 3.1664960431498913e-10\n",
      "  batch 8700 loss: 0.0\n",
      "  batch 8800 loss: 0.0\n",
      "  batch 8900 loss: 0.0\n",
      "  batch 9000 loss: 0.0\n",
      "  batch 9100 loss: 0.0\n",
      "  batch 9200 loss: 3.911553747570906e-10\n",
      "  batch 9300 loss: 3.7252895879191784e-10\n",
      "  batch 9400 loss: 0.0\n",
      "  batch 9500 loss: 0.0\n",
      "LOSS train 0.0 valid 1.3663725242302928e-09\n",
      "Testing\n",
      "sub-033\n",
      "    Mean accuracy per subject: 98.3751846381093\n",
      "sub-064\n",
      "    Mean accuracy per subject: 99.8435054773083\n",
      "sub-085\n",
      "    Mean accuracy per subject: 99.6415770609319\n",
      "sub-001\n",
      "    Mean accuracy per subject: 99.39467312348668\n",
      "sub-024\n",
      "    Mean accuracy per subject: 99.39467312348668\n",
      "sub-005\n",
      "    Mean accuracy per subject: 99.73297730307075\n",
      "sub-030\n",
      "    Mean accuracy per subject: 99.84177215189874\n",
      "sub-029\n",
      "    Mean accuracy per subject: 99.20886075949367\n",
      "sub-042\n",
      "    Mean accuracy per subject: 99.6742671009772\n",
      "sub-072\n",
      "    Mean accuracy per subject: 99.63570127504553\n",
      "sub-014\n",
      "    Mean accuracy per subject: 99.75460122699387\n",
      "sub-017\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-073\n",
      "    Mean accuracy per subject: 99.63570127504553\n",
      "sub-076\n",
      "    Mean accuracy per subject: 98.72495446265938\n",
      "sub-078\n",
      "    Mean accuracy per subject: 99.4535519125683\n",
      "sub-011\n",
      "    Mean accuracy per subject: 99.0314769975787\n",
      "sub-022\n",
      "    Mean accuracy per subject: 99.6319018404908\n",
      "sub-021\n",
      "    Mean accuracy per subject: 99.50920245398773\n",
      "sub-036\n",
      "    Mean accuracy per subject: 98.81831610044313\n",
      "sub-013\n",
      "    Mean accuracy per subject: 99.39467312348668\n",
      "sub-075\n",
      "    Mean accuracy per subject: 99.08925318761385\n",
      "sub-062\n",
      "    Mean accuracy per subject: 99.67266775777414\n",
      "Score:  99.42997692511139\n"
     ]
    }
   ],
   "source": [
    "model = Model(use_embeddings=True)\n",
    "trainer = Trainer(\n",
    "    model, train_files, val_files, test_files, args, torch.optim.Adam(model.parameters(), lr=1e-3), use_embeddings=True, embedding_type=\"wav2vec\" \n",
    ")\n",
    "print(\"Training\")\n",
    "trainer.train_model(epochs=2, run_name=\"Wav2Vec\", eps=1e-5)\n",
    "print(\"Testing\")\n",
    "trainer.test(window_length, hop_length, number_of_mismatch, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовый энкодер ЭЭГ + Эмбеддинги `Whisper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "EPOCH 1:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "  batch 400 loss: 0.0\n",
      "  batch 500 loss: 0.0\n",
      "  batch 600 loss: 0.0\n",
      "  batch 700 loss: 0.0\n",
      "  batch 800 loss: 0.0\n",
      "  batch 900 loss: 0.0\n",
      "  batch 1000 loss: 0.0\n",
      "  batch 1100 loss: 0.0\n",
      "  batch 1200 loss: 0.0\n",
      "  batch 1300 loss: 0.0\n",
      "  batch 1400 loss: 0.0\n",
      "  batch 1500 loss: 0.0\n",
      "  batch 1600 loss: 0.0\n",
      "  batch 1700 loss: 0.0\n",
      "  batch 1800 loss: 0.0\n",
      "  batch 1900 loss: 0.0\n",
      "  batch 2000 loss: 0.0\n",
      "  batch 2100 loss: 0.0\n",
      "  batch 2200 loss: 0.0\n",
      "  batch 2300 loss: 0.0\n",
      "  batch 2400 loss: 0.0\n",
      "  batch 2500 loss: 0.0\n",
      "  batch 2600 loss: 0.0\n",
      "  batch 2700 loss: 0.0\n",
      "  batch 2800 loss: 0.0\n",
      "  batch 2900 loss: 0.0\n",
      "  batch 3000 loss: 0.0\n",
      "  batch 3100 loss: 0.0\n",
      "  batch 3200 loss: 0.0\n",
      "  batch 3300 loss: 0.0\n",
      "  batch 3400 loss: 0.0\n",
      "  batch 3500 loss: 0.0\n",
      "  batch 3600 loss: 0.0\n",
      "  batch 3700 loss: 0.0\n",
      "  batch 3800 loss: 0.0\n",
      "  batch 3900 loss: 0.0\n",
      "  batch 4000 loss: 0.0\n",
      "  batch 4100 loss: 0.0\n",
      "  batch 4200 loss: 0.0\n",
      "  batch 4300 loss: 0.0\n",
      "  batch 4400 loss: 0.0\n",
      "  batch 4500 loss: 0.0\n",
      "  batch 4600 loss: 0.0\n",
      "  batch 4700 loss: 0.0\n",
      "  batch 4800 loss: 0.0\n",
      "  batch 4900 loss: 0.0\n",
      "  batch 5000 loss: 0.0\n",
      "  batch 5100 loss: 0.0\n",
      "  batch 5200 loss: 0.0\n",
      "  batch 5300 loss: 0.0\n",
      "  batch 5400 loss: 0.0\n",
      "  batch 5500 loss: 0.0\n",
      "  batch 5600 loss: 0.0\n",
      "  batch 5700 loss: 0.0\n",
      "  batch 5800 loss: 0.0\n",
      "  batch 5900 loss: 0.0\n",
      "  batch 6000 loss: 0.0\n",
      "  batch 6100 loss: 0.0\n",
      "  batch 6200 loss: 0.0\n",
      "  batch 6300 loss: 0.0\n",
      "  batch 6400 loss: 0.0\n",
      "  batch 6500 loss: 0.0\n",
      "  batch 6600 loss: 0.0\n",
      "  batch 6700 loss: 0.0\n",
      "  batch 6800 loss: 0.0\n",
      "  batch 6900 loss: 0.0\n",
      "  batch 7000 loss: 0.0\n",
      "  batch 7100 loss: 0.0\n",
      "  batch 7200 loss: 0.0\n",
      "  batch 7300 loss: 0.0\n",
      "  batch 7400 loss: 0.0\n",
      "  batch 7500 loss: 0.0\n",
      "  batch 7600 loss: 0.0\n",
      "  batch 7700 loss: 0.0\n",
      "  batch 7800 loss: 0.0\n",
      "  batch 7900 loss: 0.0\n",
      "  batch 8000 loss: 0.0\n",
      "  batch 8100 loss: 0.0\n",
      "  batch 8200 loss: 0.0\n",
      "  batch 8300 loss: 0.0\n",
      "  batch 8400 loss: 0.0\n",
      "  batch 8500 loss: 0.0\n",
      "  batch 8600 loss: 0.0\n",
      "  batch 8700 loss: 0.0\n",
      "  batch 8800 loss: 0.0\n",
      "  batch 8900 loss: 0.0\n",
      "  batch 9000 loss: 0.0\n",
      "  batch 9100 loss: 0.0\n",
      "  batch 9200 loss: 0.0\n",
      "  batch 9300 loss: 0.0\n",
      "  batch 9400 loss: 0.0\n",
      "  batch 9500 loss: 0.0\n",
      "LOSS train 0.0 valid 0.0\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "  batch 400 loss: 0.0\n",
      "  batch 500 loss: 0.0\n",
      "  batch 600 loss: 0.0\n",
      "  batch 700 loss: 0.0\n",
      "  batch 800 loss: 0.0\n",
      "  batch 900 loss: 0.0\n",
      "  batch 1000 loss: 0.0\n",
      "  batch 1100 loss: 0.0\n",
      "  batch 1200 loss: 0.0\n",
      "  batch 1300 loss: 0.0\n",
      "  batch 1400 loss: 0.0\n",
      "  batch 1500 loss: 0.0\n",
      "  batch 1600 loss: 0.0\n",
      "  batch 1700 loss: 0.0\n",
      "  batch 1800 loss: 0.0\n",
      "  batch 1900 loss: 0.0\n",
      "  batch 2000 loss: 0.0\n",
      "  batch 2100 loss: 0.0\n",
      "  batch 2200 loss: 0.0\n",
      "  batch 2300 loss: 0.0\n",
      "  batch 2400 loss: 0.0\n",
      "  batch 2500 loss: 0.0\n",
      "  batch 2600 loss: 0.0\n",
      "  batch 2700 loss: 0.0\n",
      "  batch 2800 loss: 0.0\n",
      "  batch 2900 loss: 0.0\n",
      "  batch 3000 loss: 0.0\n",
      "  batch 3100 loss: 0.0\n",
      "  batch 3200 loss: 0.0\n",
      "  batch 3300 loss: 0.0\n",
      "  batch 3400 loss: 0.0\n",
      "  batch 3500 loss: 0.0\n",
      "  batch 3600 loss: 0.0\n",
      "  batch 3700 loss: 0.0\n",
      "  batch 3800 loss: 0.0\n",
      "  batch 3900 loss: 0.0\n",
      "  batch 4000 loss: 0.0\n",
      "  batch 4100 loss: 0.0\n",
      "  batch 4200 loss: 0.0\n",
      "  batch 4300 loss: 0.0\n",
      "  batch 4400 loss: 0.0\n",
      "  batch 4500 loss: 0.0\n",
      "  batch 4600 loss: 0.0\n",
      "  batch 4700 loss: 0.0\n",
      "  batch 4800 loss: 0.0\n",
      "  batch 4900 loss: 0.0\n",
      "  batch 5000 loss: 0.0\n",
      "  batch 5100 loss: 0.0\n",
      "  batch 5200 loss: 0.0\n",
      "  batch 5300 loss: 0.0\n",
      "  batch 5400 loss: 0.0\n",
      "  batch 5500 loss: 0.0\n",
      "  batch 5600 loss: 0.0\n",
      "  batch 5700 loss: 0.0\n",
      "  batch 5800 loss: 0.0\n",
      "  batch 5900 loss: 0.0\n",
      "  batch 6000 loss: 0.0\n",
      "  batch 6100 loss: 0.0\n",
      "  batch 6200 loss: 0.0\n",
      "  batch 6300 loss: 0.0\n",
      "  batch 6400 loss: 0.0\n",
      "  batch 6500 loss: 0.0\n",
      "  batch 6600 loss: 0.0\n",
      "  batch 6700 loss: 0.0\n",
      "  batch 6800 loss: 0.0\n",
      "  batch 6900 loss: 0.0\n",
      "  batch 7000 loss: 0.0\n",
      "  batch 7100 loss: 0.0\n",
      "  batch 7200 loss: 0.0\n",
      "  batch 7300 loss: 0.0\n",
      "  batch 7400 loss: 0.0\n",
      "  batch 7500 loss: 0.0\n",
      "  batch 7600 loss: 0.0\n",
      "  batch 7700 loss: 0.0\n",
      "  batch 7800 loss: 0.0\n",
      "  batch 7900 loss: 0.0\n",
      "  batch 8000 loss: 0.0\n",
      "  batch 8100 loss: 0.0\n",
      "  batch 8200 loss: 0.0\n",
      "  batch 8300 loss: 0.0\n",
      "  batch 8400 loss: 0.0\n",
      "  batch 8500 loss: 0.0\n",
      "  batch 8600 loss: 0.0\n",
      "  batch 8700 loss: 0.0\n",
      "  batch 8800 loss: 0.0\n",
      "  batch 8900 loss: 0.0\n",
      "  batch 9000 loss: 0.0\n",
      "  batch 9100 loss: 0.0\n",
      "  batch 9200 loss: 0.0\n",
      "  batch 9300 loss: 0.0\n",
      "  batch 9400 loss: 0.0\n",
      "  batch 9500 loss: 0.0\n",
      "LOSS train 0.0 valid 0.0\n",
      "Testing\n",
      "sub-029\n",
      "    Mean accuracy per subject: 83.38607594936708\n",
      "sub-072\n",
      "    Mean accuracy per subject: 87.97814207650273\n",
      "sub-005\n",
      "    Mean accuracy per subject: 88.78504672897196\n",
      "sub-076\n",
      "    Mean accuracy per subject: 82.69581056466302\n",
      "sub-075\n",
      "    Mean accuracy per subject: 89.61748633879782\n",
      "sub-033\n",
      "    Mean accuracy per subject: 79.91137370753323\n",
      "sub-042\n",
      "    Mean accuracy per subject: 80.78175895765472\n",
      "sub-064\n",
      "    Mean accuracy per subject: 90.61032863849765\n",
      "sub-011\n",
      "    Mean accuracy per subject: 79.41888619854721\n",
      "sub-030\n",
      "    Mean accuracy per subject: 79.5886075949367\n",
      "sub-062\n",
      "    Mean accuracy per subject: 87.56137479541735\n",
      "sub-036\n",
      "    Mean accuracy per subject: 79.91137370753323\n",
      "sub-024\n",
      "    Mean accuracy per subject: 83.53510895883777\n",
      "sub-013\n",
      "    Mean accuracy per subject: 83.17191283292978\n",
      "sub-085\n",
      "    Mean accuracy per subject: 79.92831541218638\n",
      "sub-022\n",
      "    Mean accuracy per subject: 74.60122699386503\n",
      "sub-001\n",
      "    Mean accuracy per subject: 76.63438256658596\n",
      "sub-017\n",
      "    Mean accuracy per subject: 89.0282131661442\n",
      "sub-021\n",
      "    Mean accuracy per subject: 85.2760736196319\n",
      "sub-014\n",
      "    Mean accuracy per subject: 80.61349693251533\n",
      "sub-073\n",
      "    Mean accuracy per subject: 81.9672131147541\n",
      "sub-078\n",
      "    Mean accuracy per subject: 87.7959927140255\n",
      "Score:  83.30900916226813\n"
     ]
    }
   ],
   "source": [
    "model = Model(use_embeddings=True)\n",
    "trainer = Trainer(\n",
    "    model, train_files, val_files, test_files, args, torch.optim.Adam(model.parameters(), lr=1e-3), use_embeddings=True, embedding_type=\"whisper\"\n",
    ")\n",
    "print(\"Training\")\n",
    "trainer.train_model(epochs=2, run_name=\"Whisper\", eps=1e-5)\n",
    "print(\"Testing\")\n",
    "trainer.test(window_length, hop_length, number_of_mismatch, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформер для ЭЭГ + Эмбеддинги `Wav2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "EPOCH 1:\n",
      "  batch 100 loss: 0.0012281817494652746\n",
      "  batch 200 loss: 1.06697241101672e-05\n",
      "  batch 300 loss: 0.00010448084200857188\n",
      "  batch 400 loss: 3.885615478793625e-07\n",
      "  batch 500 loss: 1.3007353100391583e-07\n",
      "  batch 600 loss: 1.6763799237651256e-10\n",
      "  batch 700 loss: 4.1924858962261166e-08\n",
      "  batch 800 loss: 1.368063263385011e-07\n",
      "  batch 900 loss: 1.7764142739018497e-07\n",
      "  batch 1000 loss: 8.501882689415651e-07\n",
      "  batch 1100 loss: 2.497818298934362e-06\n",
      "  batch 1200 loss: 1.1581593201981377e-06\n",
      "  batch 1300 loss: 1.8590069304047496e-07\n",
      "  batch 1400 loss: 1.5280927270211465e-07\n",
      "  batch 1500 loss: 9.576736552219245e-08\n",
      "  batch 1600 loss: 2.9988486283105685e-09\n",
      "  batch 1700 loss: 5.027000229418377e-08\n",
      "  batch 1800 loss: 1.111972579792564e-08\n",
      "  batch 1900 loss: 1.1008106194365652e-08\n",
      "  batch 2000 loss: 1.1175870007207323e-10\n",
      "  batch 2100 loss: 0.0\n",
      "  batch 2200 loss: 0.0\n",
      "  batch 2300 loss: 0.0\n",
      "  batch 2400 loss: 1.303851426825986e-10\n",
      "  batch 2500 loss: 0.0\n",
      "  batch 2600 loss: 0.0\n",
      "  batch 2700 loss: 0.0\n",
      "  batch 2800 loss: 7.45057704421015e-10\n",
      "  batch 2900 loss: 0.0\n",
      "  batch 3000 loss: 1.8626450382086545e-11\n",
      "  batch 3100 loss: 0.0\n",
      "  batch 3200 loss: 3.7252895879191784e-10\n",
      "  batch 3300 loss: 0.0\n",
      "  batch 3400 loss: 0.0\n",
      "  batch 3500 loss: 0.0\n",
      "  batch 3600 loss: 0.0\n",
      "  batch 3700 loss: 0.0\n",
      "  batch 3800 loss: 5.587933671336032e-10\n",
      "  batch 3900 loss: 0.0\n",
      "  batch 4000 loss: 0.0\n",
      "  batch 4100 loss: 0.0\n",
      "  batch 4200 loss: 0.0\n",
      "  batch 4300 loss: 0.0\n",
      "  batch 4400 loss: 0.0\n",
      "  batch 4500 loss: 0.0\n",
      "  batch 4600 loss: 0.0\n",
      "  batch 4700 loss: 0.0\n",
      "  batch 4800 loss: 1.4901159417490818e-10\n",
      "  batch 4900 loss: 0.0\n",
      "  batch 5000 loss: 0.0\n",
      "  batch 5100 loss: 0.0\n",
      "  batch 5200 loss: 0.0\n",
      "  batch 5300 loss: 0.0\n",
      "  batch 5400 loss: 0.0\n",
      "  batch 5500 loss: 0.0\n",
      "  batch 5600 loss: 7.450580152834618e-11\n",
      "  batch 5700 loss: 0.0\n",
      "  batch 5800 loss: 7.823105896420657e-10\n",
      "  batch 5900 loss: 1.7508850191916281e-09\n",
      "  batch 6000 loss: 1.8626450382086545e-11\n",
      "  batch 6100 loss: 1.5646193674001552e-09\n",
      "  batch 6200 loss: 9.77884383246419e-09\n",
      "  batch 6300 loss: 7.450580152834618e-11\n",
      "  batch 6400 loss: 1.0272144209011458e-07\n",
      "  batch 6500 loss: 0.0\n",
      "  batch 6600 loss: 0.0\n",
      "  batch 6700 loss: 0.0\n",
      "  batch 6800 loss: 3.725290076417309e-11\n",
      "  batch 6900 loss: 0.0\n",
      "  batch 7000 loss: 0.0\n",
      "  batch 7100 loss: 0.0\n",
      "  batch 7200 loss: 0.0\n",
      "  batch 7300 loss: 0.0\n",
      "  batch 7400 loss: 1.8626450382086545e-11\n",
      "  batch 7500 loss: 5.774197120445024e-10\n",
      "  batch 7600 loss: 0.0\n",
      "  batch 7700 loss: 0.0\n",
      "  batch 7800 loss: 5.774197120445024e-10\n",
      "  batch 7900 loss: 1.6204981534428952e-09\n",
      "  batch 8000 loss: 0.0\n",
      "  batch 8100 loss: 0.0\n",
      "  batch 8200 loss: 0.0\n",
      "  batch 8300 loss: 0.0\n",
      "  batch 8400 loss: 5.587935003603661e-11\n",
      "  batch 8500 loss: 0.0\n",
      "  batch 8600 loss: 0.0\n",
      "  batch 8700 loss: 1.303851426825986e-10\n",
      "  batch 8800 loss: 0.0\n",
      "  batch 8900 loss: 0.0\n",
      "  batch 9000 loss: 0.0\n",
      "  batch 9100 loss: 0.0\n",
      "  batch 9200 loss: 0.0\n",
      "  batch 9300 loss: 0.0\n",
      "  batch 9400 loss: 0.0\n",
      "  batch 9500 loss: 0.0\n",
      "LOSS train 0.0 valid 1.061102954455877e-10\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "  batch 400 loss: 0.0\n",
      "  batch 500 loss: 0.0\n",
      "  batch 600 loss: 0.0\n",
      "  batch 700 loss: 0.0\n",
      "  batch 800 loss: 0.0\n",
      "  batch 900 loss: 0.0\n",
      "  batch 1000 loss: 0.0\n",
      "  batch 1100 loss: 0.0\n",
      "  batch 1200 loss: 0.0\n",
      "  batch 1300 loss: 0.0\n",
      "  batch 1400 loss: 3.7252898543727045e-11\n",
      "  batch 1500 loss: 1.1175867342672064e-10\n",
      "  batch 1600 loss: 0.0\n",
      "  batch 1700 loss: 0.0\n",
      "  batch 1800 loss: 0.0\n",
      "  batch 1900 loss: 0.0\n",
      "  batch 2000 loss: 0.0\n",
      "  batch 2100 loss: 0.0\n",
      "  batch 2200 loss: 0.0\n",
      "  batch 2300 loss: 0.0\n",
      "  batch 2400 loss: 0.0\n",
      "  batch 2500 loss: 0.0\n",
      "  batch 2600 loss: 0.0\n",
      "  batch 2700 loss: 0.0\n",
      "  batch 2800 loss: 0.0\n",
      "  batch 2900 loss: 0.0\n",
      "  batch 3000 loss: 0.0\n",
      "  batch 3100 loss: 0.0\n",
      "  batch 3200 loss: 0.0\n",
      "  batch 3300 loss: 0.0\n",
      "  batch 3400 loss: 0.0\n",
      "  batch 3500 loss: 0.0\n",
      "  batch 3600 loss: 0.0\n",
      "  batch 3700 loss: 0.0\n",
      "  batch 3800 loss: 0.0\n",
      "  batch 3900 loss: 0.0\n",
      "  batch 4000 loss: 0.0\n",
      "  batch 4100 loss: 0.0\n",
      "  batch 4200 loss: 0.0\n",
      "  batch 4300 loss: 0.0\n",
      "  batch 4400 loss: 0.0\n",
      "  batch 4500 loss: 0.0\n",
      "  batch 4600 loss: 0.0\n",
      "  batch 4700 loss: 0.0\n",
      "  batch 4800 loss: 0.0\n",
      "  batch 4900 loss: 0.0\n",
      "  batch 5000 loss: 0.0\n",
      "  batch 5100 loss: 0.0\n",
      "  batch 5200 loss: 0.0\n",
      "  batch 5300 loss: 0.0\n",
      "  batch 5400 loss: 0.0\n",
      "  batch 5500 loss: 0.0\n",
      "  batch 5600 loss: 0.0\n",
      "  batch 5700 loss: 0.0\n",
      "  batch 5800 loss: 0.0\n",
      "  batch 5900 loss: 0.0\n",
      "  batch 6000 loss: 0.0\n",
      "  batch 6100 loss: 0.0\n",
      "  batch 6200 loss: 0.0\n",
      "  batch 6300 loss: 0.0\n",
      "  batch 6400 loss: 0.0\n",
      "  batch 6500 loss: 0.0\n",
      "  batch 6600 loss: 0.0\n",
      "  batch 6700 loss: 0.0\n",
      "  batch 6800 loss: 0.0\n",
      "  batch 6900 loss: 0.0\n",
      "  batch 7000 loss: 0.0\n",
      "  batch 7100 loss: 0.0\n",
      "  batch 7200 loss: 0.0\n",
      "  batch 7300 loss: 0.0\n",
      "  batch 7400 loss: 0.0\n",
      "  batch 7500 loss: 3.725290076417309e-11\n",
      "  batch 7600 loss: 0.0\n",
      "  batch 7700 loss: 0.0\n",
      "  batch 7800 loss: 0.0\n",
      "  batch 7900 loss: 0.0\n",
      "  batch 8000 loss: 0.0\n",
      "  batch 8100 loss: 0.0\n",
      "  batch 8200 loss: 0.0\n",
      "  batch 8300 loss: 0.0\n",
      "  batch 8400 loss: 0.0\n",
      "  batch 8500 loss: 0.0\n",
      "  batch 8600 loss: 0.0\n",
      "  batch 8700 loss: 0.0\n",
      "  batch 8800 loss: 0.0\n",
      "  batch 8900 loss: 0.0\n",
      "  batch 9000 loss: 0.0\n",
      "  batch 9100 loss: 0.0\n",
      "  batch 9200 loss: 0.0\n",
      "  batch 9300 loss: 0.0\n",
      "  batch 9400 loss: 0.0\n",
      "  batch 9500 loss: 0.0\n",
      "LOSS train 0.0 valid 0.0\n",
      "Testing\n",
      "sub-017\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-085\n",
      "    Mean accuracy per subject: 99.46236559139786\n",
      "sub-033\n",
      "    Mean accuracy per subject: 99.70457902511079\n",
      "sub-001\n",
      "    Mean accuracy per subject: 99.75786924939467\n",
      "sub-030\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-036\n",
      "    Mean accuracy per subject: 99.70457902511079\n",
      "sub-072\n",
      "    Mean accuracy per subject: 99.81785063752277\n",
      "sub-013\n",
      "    Mean accuracy per subject: 99.87893462469734\n",
      "sub-073\n",
      "    Mean accuracy per subject: 99.63570127504553\n",
      "sub-005\n",
      "    Mean accuracy per subject: 99.86648865153538\n",
      "sub-064\n",
      "    Mean accuracy per subject: 99.68701095461658\n",
      "sub-042\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-076\n",
      "    Mean accuracy per subject: 99.81785063752277\n",
      "sub-078\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-075\n",
      "    Mean accuracy per subject: 99.81785063752277\n",
      "sub-022\n",
      "    Mean accuracy per subject: 99.75460122699387\n",
      "sub-024\n",
      "    Mean accuracy per subject: 99.75786924939467\n",
      "sub-021\n",
      "    Mean accuracy per subject: 99.87730061349693\n",
      "sub-062\n",
      "    Mean accuracy per subject: 99.34533551554829\n",
      "sub-011\n",
      "    Mean accuracy per subject: 99.63680387409201\n",
      "sub-014\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-029\n",
      "    Mean accuracy per subject: 99.68354430379746\n",
      "Score:  99.78211523149093\n"
     ]
    }
   ],
   "source": [
    "model = Model(use_transformer=True, use_embeddings=True)\n",
    "trainer = Trainer(\n",
    "    model, train_files, val_files, test_files, args, torch.optim.Adam(model.parameters(), lr=1e-3), use_embeddings=True, embedding_type=\"wav2vec\"\n",
    ")\n",
    "print(\"Training\")\n",
    "trainer.train_model(epochs=2, run_name=\"Transformer_Wav2Vec\", eps=1e-5)\n",
    "print(\"Testing\")\n",
    "trainer.test(window_length, hop_length, number_of_mismatch, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформер для ЭЭГ + Эмбеддинги `Whisper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "EPOCH 1:\n",
      "  batch 100 loss: 0.4832695484161377\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "  batch 400 loss: 0.0\n",
      "  batch 500 loss: 0.0\n",
      "  batch 600 loss: 0.0\n",
      "  batch 700 loss: 0.0\n",
      "  batch 800 loss: 0.0\n",
      "  batch 900 loss: 0.0\n",
      "  batch 1000 loss: 0.0\n",
      "  batch 1100 loss: 0.0\n",
      "  batch 1200 loss: 0.0\n",
      "  batch 1300 loss: 0.0\n",
      "  batch 1400 loss: 0.0\n",
      "  batch 1500 loss: 0.0\n",
      "  batch 1600 loss: 0.0\n",
      "  batch 1700 loss: 0.0\n",
      "  batch 1800 loss: 0.0\n",
      "  batch 1900 loss: 0.0\n",
      "  batch 2000 loss: 0.0\n",
      "  batch 2100 loss: 0.0\n",
      "  batch 2200 loss: 0.0\n",
      "  batch 2300 loss: 0.0\n",
      "  batch 2400 loss: 0.0\n",
      "  batch 2500 loss: 0.0\n",
      "  batch 2600 loss: 0.0\n",
      "  batch 2700 loss: 0.0\n",
      "  batch 2800 loss: 0.0\n",
      "  batch 2900 loss: 0.0\n",
      "  batch 3000 loss: 0.0\n",
      "  batch 3100 loss: 0.0\n",
      "  batch 3200 loss: 0.0\n",
      "  batch 3300 loss: 0.0\n",
      "  batch 3400 loss: 0.0\n",
      "  batch 3500 loss: 0.0\n",
      "  batch 3600 loss: 0.0\n",
      "  batch 3700 loss: 0.0\n",
      "  batch 3800 loss: 0.0\n",
      "  batch 3900 loss: 0.0\n",
      "  batch 4000 loss: 0.0\n",
      "  batch 4100 loss: 0.0\n",
      "  batch 4200 loss: 0.0\n",
      "  batch 4300 loss: 0.0\n",
      "  batch 4400 loss: 0.0\n",
      "  batch 4500 loss: 0.0\n",
      "  batch 4600 loss: 0.0\n",
      "  batch 4700 loss: 0.0\n",
      "  batch 4800 loss: 0.0\n",
      "  batch 4900 loss: 0.0\n",
      "  batch 5000 loss: 0.0\n",
      "  batch 5100 loss: 0.0\n",
      "  batch 5200 loss: 0.0\n",
      "  batch 5300 loss: 0.0\n",
      "  batch 5400 loss: 0.0\n",
      "  batch 5500 loss: 0.0\n",
      "  batch 5600 loss: 0.0\n",
      "  batch 5700 loss: 0.0\n",
      "  batch 5800 loss: 0.0\n",
      "  batch 5900 loss: 0.0\n",
      "  batch 6000 loss: 0.0\n",
      "  batch 6100 loss: 0.0\n",
      "  batch 6200 loss: 0.0\n",
      "  batch 6300 loss: 0.0\n",
      "  batch 6400 loss: 0.0\n",
      "  batch 6500 loss: 0.0\n",
      "  batch 6600 loss: 0.0\n",
      "  batch 6700 loss: 0.0\n",
      "  batch 6800 loss: 0.0\n",
      "  batch 6900 loss: 0.0\n",
      "  batch 7000 loss: 0.0\n",
      "  batch 7100 loss: 0.0\n",
      "  batch 7200 loss: 0.0\n",
      "  batch 7300 loss: 0.0\n",
      "  batch 7400 loss: 0.0\n",
      "  batch 7500 loss: 0.0\n",
      "  batch 7600 loss: 0.0\n",
      "  batch 7700 loss: 0.0\n",
      "  batch 7800 loss: 0.0\n",
      "  batch 7900 loss: 0.0\n",
      "  batch 8000 loss: 0.0\n",
      "  batch 8100 loss: 0.0\n",
      "  batch 8200 loss: 0.0\n",
      "  batch 8300 loss: 0.0\n",
      "  batch 8400 loss: 0.0\n",
      "  batch 8500 loss: 0.0\n",
      "  batch 8600 loss: 0.0\n",
      "  batch 8700 loss: 0.0\n",
      "  batch 8800 loss: 0.0\n",
      "  batch 8900 loss: 0.0\n",
      "  batch 9000 loss: 0.0\n",
      "  batch 9100 loss: 0.0\n",
      "  batch 9200 loss: 0.0\n",
      "  batch 9300 loss: 0.0\n",
      "  batch 9400 loss: 0.0\n",
      "  batch 9500 loss: 0.0\n",
      "LOSS train 0.0 valid 0.0\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.0\n",
      "  batch 200 loss: 0.0\n",
      "  batch 300 loss: 0.0\n",
      "  batch 400 loss: 0.0\n",
      "  batch 500 loss: 0.0\n",
      "  batch 600 loss: 0.0\n",
      "  batch 700 loss: 0.0\n",
      "  batch 800 loss: 0.0\n",
      "  batch 900 loss: 0.0\n",
      "  batch 1000 loss: 0.0\n",
      "  batch 1100 loss: 0.0\n",
      "  batch 1200 loss: 0.0\n",
      "  batch 1300 loss: 0.0\n",
      "  batch 1400 loss: 0.0\n",
      "  batch 1500 loss: 0.0\n",
      "  batch 1600 loss: 0.0\n",
      "  batch 1700 loss: 0.0\n",
      "  batch 1800 loss: 0.0\n",
      "  batch 1900 loss: 0.0\n",
      "  batch 2000 loss: 0.0\n",
      "  batch 2100 loss: 0.0\n",
      "  batch 2200 loss: 0.0\n",
      "  batch 2300 loss: 0.0\n",
      "  batch 2400 loss: 0.0\n",
      "  batch 2500 loss: 0.0\n",
      "  batch 2600 loss: 0.0\n",
      "  batch 2700 loss: 0.0\n",
      "  batch 2800 loss: 0.0\n",
      "  batch 2900 loss: 0.0\n",
      "  batch 3000 loss: 0.0\n",
      "  batch 3100 loss: 0.0\n",
      "  batch 3200 loss: 0.0\n",
      "  batch 3300 loss: 0.0\n",
      "  batch 3400 loss: 0.0\n",
      "  batch 3500 loss: 0.0\n",
      "  batch 3600 loss: 0.0\n",
      "  batch 3700 loss: 0.0\n",
      "  batch 3800 loss: 0.0\n",
      "  batch 3900 loss: 0.0\n",
      "  batch 4000 loss: 0.0\n",
      "  batch 4100 loss: 0.0\n",
      "  batch 4200 loss: 0.0\n",
      "  batch 4300 loss: 0.0\n",
      "  batch 4400 loss: 0.0\n",
      "  batch 4500 loss: 0.0\n",
      "  batch 4600 loss: 0.0\n",
      "  batch 4700 loss: 0.0\n",
      "  batch 4800 loss: 0.0\n",
      "  batch 4900 loss: 0.0\n",
      "  batch 5000 loss: 0.0\n",
      "  batch 5100 loss: 0.0\n",
      "  batch 5200 loss: 0.0\n",
      "  batch 5300 loss: 0.0\n",
      "  batch 5400 loss: 0.0\n",
      "  batch 5500 loss: 0.0\n",
      "  batch 5600 loss: 0.0\n",
      "  batch 5700 loss: 0.0\n",
      "  batch 5800 loss: 0.0\n",
      "  batch 5900 loss: 0.0\n",
      "  batch 6000 loss: 0.0\n",
      "  batch 6100 loss: 0.0\n",
      "  batch 6200 loss: 0.0\n",
      "  batch 6300 loss: 0.0\n",
      "  batch 6400 loss: 0.0\n",
      "  batch 6500 loss: 0.0\n",
      "  batch 6600 loss: 0.0\n",
      "  batch 6700 loss: 0.0\n",
      "  batch 6800 loss: 0.0\n",
      "  batch 6900 loss: 0.0\n",
      "  batch 7000 loss: 0.0\n",
      "  batch 7100 loss: 0.0\n",
      "  batch 7200 loss: 0.0\n",
      "  batch 7300 loss: 0.0\n",
      "  batch 7400 loss: 0.0\n",
      "  batch 7500 loss: 0.0\n",
      "  batch 7600 loss: 0.0\n",
      "  batch 7700 loss: 0.0\n",
      "  batch 7800 loss: 0.0\n",
      "  batch 7900 loss: 0.0\n",
      "  batch 8000 loss: 0.0\n",
      "  batch 8100 loss: 0.0\n",
      "  batch 8200 loss: 0.0\n",
      "  batch 8300 loss: 0.0\n",
      "  batch 8400 loss: 0.0\n",
      "  batch 8500 loss: 0.0\n",
      "  batch 8600 loss: 0.0\n",
      "  batch 8700 loss: 0.0\n",
      "  batch 8800 loss: 0.0\n",
      "  batch 8900 loss: 0.0\n",
      "  batch 9000 loss: 0.0\n",
      "  batch 9100 loss: 0.0\n",
      "  batch 9200 loss: 0.0\n",
      "  batch 9300 loss: 0.0\n",
      "  batch 9400 loss: 0.0\n",
      "  batch 9500 loss: 0.0\n",
      "LOSS train 0.0 valid 0.0\n",
      "Testing\n",
      "sub-001\n",
      "    Mean accuracy per subject: 96.97336561743342\n",
      "sub-062\n",
      "    Mean accuracy per subject: 91.81669394435352\n",
      "sub-085\n",
      "    Mean accuracy per subject: 92.29390681003584\n",
      "sub-011\n",
      "    Mean accuracy per subject: 96.97336561743342\n",
      "sub-022\n",
      "    Mean accuracy per subject: 98.15950920245399\n",
      "sub-064\n",
      "    Mean accuracy per subject: 93.74021909233177\n",
      "sub-076\n",
      "    Mean accuracy per subject: 93.0783242258652\n",
      "sub-024\n",
      "    Mean accuracy per subject: 96.73123486682809\n",
      "sub-014\n",
      "    Mean accuracy per subject: 99.14110429447852\n",
      "sub-073\n",
      "    Mean accuracy per subject: 92.89617486338798\n",
      "sub-017\n",
      "    Mean accuracy per subject: 100.0\n",
      "sub-075\n",
      "    Mean accuracy per subject: 94.35336976320583\n",
      "sub-030\n",
      "    Mean accuracy per subject: 93.67088607594937\n",
      "sub-013\n",
      "    Mean accuracy per subject: 97.82082324455206\n",
      "sub-005\n",
      "    Mean accuracy per subject: 98.26435246995995\n",
      "sub-078\n",
      "    Mean accuracy per subject: 94.1712204007286\n",
      "sub-029\n",
      "    Mean accuracy per subject: 93.19620253164557\n",
      "sub-021\n",
      "    Mean accuracy per subject: 98.8957055214724\n",
      "sub-072\n",
      "    Mean accuracy per subject: 95.81056466302368\n",
      "sub-042\n",
      "    Mean accuracy per subject: 91.69381107491857\n",
      "sub-036\n",
      "    Mean accuracy per subject: 94.23929098966026\n",
      "sub-033\n",
      "    Mean accuracy per subject: 95.86410635155096\n",
      "Score:  95.44473780096678\n"
     ]
    }
   ],
   "source": [
    "model = Model(use_transformer=True, use_embeddings=True)\n",
    "trainer = Trainer(\n",
    "    model, train_files, val_files, test_files, args, torch.optim.Adam(model.parameters(), lr=1e-3), use_embeddings=True, embedding_type=\"whisper\"\n",
    ")\n",
    "print(\"Training\")\n",
    "trainer.train_model(epochs=2, run_name=\"Transformer_Whisper\", eps=1e-5)\n",
    "print(\"Testing\")\n",
    "trainer.test(window_length, hop_length, number_of_mismatch, None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
